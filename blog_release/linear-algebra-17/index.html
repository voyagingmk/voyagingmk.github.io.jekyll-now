<!DOCTYPE html>
<html>
  <head>
    <title>线性代数之主成分分析(PCA)算法 – Wyman的原创技术博客 – 恭喜你发现我的小站，撩我请加QQ：234707482、Wechat：_Wyman</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <meta name="baidu-site-verification" content="0OpfO1OtHA" />
    
    <meta name="description" content="PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。

另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。
" />
    <meta property="og:description" content="PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。

另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。
" />
    
    <meta name="author" content="Wyman的原创技术博客" />

    
    <meta property="og:title" content="线性代数之主成分分析(PCA)算法" />
    <meta property="twitter:title" content="线性代数之主成分分析(PCA)算法" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Wyman的原创技术博客 - 恭喜你发现我的小站，撩我请加QQ：234707482、Wechat：_Wyman" href="/feed.xml" />
    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-65954265-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/linear-algebra-17/',
		  'title': '线性代数之主成分分析(PCA)算法'
		});
	</script>
	<!-- End Google Analytics -->
	<!-- Baidu Analytics -->
	<script>
		var _hmt = _hmt || [];
		(function() {
		  var hm = document.createElement("script");
		  hm.src = "//hm.baidu.com/hm.js?0dc968591d8c64196a37eca9ca4f86b3";
		  var s = document.getElementsByTagName("script")[0]; 
		  s.parentNode.insertBefore(hm, s);
		})();
	</script>
	<!-- End Baidu Analytics -->

  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://www.qiujiawei.com/images/avatar.jpg" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Wyman的原创技术博客</a></h1>
            <p class="site-description">恭喜你发现我的小站，撩我请加QQ：234707482、Wechat：_Wyman</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <nav class="nav2">
      <ul></ul>
    </nav>

    <div id="main" role="main" class="container">
      <section>  
        <script src="https://code.jquery.com/jquery-3.3.0.min.js" integrity="sha256-RTQy8VOmNlT6b2PIRur37p6JEBZUE7o8wPgMvu18MC4=" crossorigin="anonymous"></script>
<script src="/main.js"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<h1>线性代数之主成分分析(PCA)算法</h1>
 <h3>Tags: <a href="/tag/matrix/" rel="tag">matrix</a>, <a href="/tag/math/" rel="tag">math</a></h3>
<article class="post">
    
    <div class="entry">
        <p>PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。</p>

<p>另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。</p>

<!--more-->

<h1>Part I</h1>

<p>设向量\( \vec x \)表示对某个特征的n次采样(测量), 那么如果有m个不同的特征，就组成了一个\(m\times n \)的矩阵\( X \)：</p>

<p>\[ X =  \left[ \begin{matrix} \vec x_{1}\\   \vec x_{2}\\   \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>然后问题来了：每个特征之间是否是<strong>相互独立(independant)</strong>的？如果是，那么说明这m个特征是良好的，可以直接拿去应用到任务中(譬如基于这些特征做一个分类器)；如果不是，那么就说明有特征是多余的，譬如\(  \vec x_{a} \)、\(  \vec x_{b} \)分别用米和英尺记录了同一个特征，虽然数值不一样，然而并没有什么卵用。</p>

<p>量化特征与特征之间的关系的最好办法是用<strong>方差</strong>(<a href="https://en.wikipedia.org/wiki/Variance">Variance</a>)和<strong>协方差</strong>(<a href="https://en.wikipedia.org/wiki/Covariance">Covariance</a>)，这2者又共同涉及到了更基础的概念<strong>数学期望</strong>(<a href="https://en.wikipedia.org/wiki/Expected_value">Expected Value</a>)和<strong>均值</strong>(<a href="https://en.wikipedia.org/wiki/Mean">Mean</a>)。先简单过一遍这4个东西的公式。</p>

<h3>数学期望和均值</h3>

<p>数学期望公式：</p>

<p>\[ E[\vec x] = \sum _{i=1}^{n}x_{i}p_{i} \]</p>

<p>当每个\( x_{i} \)的出现概率相等时(均匀分布)，有\( p_{i} = \frac {1}{n} \)，所以上式可简化成:</p>

<p>\[ E[\vec x] = \frac {1}{n}\sum _{i=1}^{n }x_{i} \]</p>

<p>上式其实也就是均值\( \overline {x} \)的定义，所以当\(x_{i}\)均匀分布时，有：</p>

<p>\[  E[\vec x] =  \overline {x} \]</p>

<p>有时候也用\( \mu \)来指代Mean。</p>

<h3>方差和协方差</h3>

<p>方差:</p>

<p>\[ Var(\vec x) = E[ (\vec x - E[\vec x])^{2 } ] = E[ (\vec x - E[\vec x])(\vec x -  E[\vec x]) ]  \]</p>

<p>协方差:</p>

<p>\[ Cov(\vec x, \vec y) = E[ (\vec x -  E[\vec x])(\vec y -  E[\vec y]) ] \]</p>

<p>可以发现方差是协方差的特殊情况:</p>

<p>\[ Var(\vec x) = Cov(\vec x, \vec x) \]</p>

<h3>协方差矩阵</h3>

<p>在<a href="http://daobiao.win:4000/linear-algebra-7/">线性代数之各种各样的矩阵</a>最后面已经提到了协方差矩阵(Covariance matrix):</p>

<p>\[ C =  \left[ \begin{matrix} E[(\vec x_{1} - \mu_{1})(\vec x_{1} - \mu_{1})]&amp;  E[(\vec x_{1} - \mu_{1})(\vec x_{2} - \mu_{2})]&amp;  \cdots &amp; E[(\vec x_{1} - \mu_{1})(\vec x_{m} - \mu_{m})]\\            E[(\vec x_{2} - \mu_{2})(\vec x_{1} - \mu_{1})]&amp;  E[(\vec x_{2} - \mu_{2})(\vec x_{2} - \mu_{2})]&amp;  \cdots &amp; E[(\vec x_{2} - \mu_{2})(\vec x_{m} - \mu_{m})]\\   \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\         E[(\vec x_{m} - \mu_{m})(\vec x_{1} - \mu_{1})]&amp;  E[(\vec x_{m} - \mu_{m})(\vec x_{2} - \mu_{2})]&amp;  \cdots &amp; E[(\vec x_{m} - \mu_{m})(\vec x_{m} - \mu_{m})]\\ \end{matrix} \right]  \]</p>

<h3>当Mean等于0时的情况</h3>

<p>当Mean等于0时，上面的协方差矩阵变成：</p>

<p>\[ C =  \left[ \begin{matrix} E[\vec x_{1}\vec x_{1}]&amp;  E[\vec x_{1} \vec x_{2}]&amp;  \cdots &amp; E[\vec x_{1}\vec x_{m}]\\            E[\vec x_{2}\vec x_{1}]&amp;  E[\vec x_{2}\vec x_{2}]&amp;  \cdots &amp; E[\vec x_{2}\vec x_{m}]\\   \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\         E[\vec x_{m}\vec x_{1}]&amp;  E[\vec x_{m}\vec x_{2}]&amp;  \cdots &amp; E[\vec x_{m}\vec x_{m}]\\ \end{matrix} \right]  \]</p>

<p>再假设\( \vec x \)每个分量的取值是均匀分布的，那么根据上面的定义，有：</p>

<p>\[E[\vec x_{a}\vec x_{b}] = \frac {1}{n}\sum _{i=1}^{n} \vec x_{ai}\vec x_{bi} , 1 \leq a\leq m, 1 \leq b\leq m  \]</p>

<p>代入上式，得到：</p>

<p>\[ C = \frac {1}{n} \left[ \begin{matrix} \sum _{i=1}^{n} \vec x_{1}\vec x_{1}&amp;  \sum _{i=1}^{n} \vec x_{1}\vec x_{2}&amp;  \cdots &amp; \sum _{i=1}^{n} \vec x_{1}\vec x_{m}\\            \sum _{i=1}^{n} \vec x_{2}\vec x_{1}&amp;  \sum _{i=1}^{n} \vec x_{2}\vec x_{2}&amp;  \cdots &amp; \sum _{i=1}^{n} \vec x_{2}\vec x_{m}\\   \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\        \sum _{i=1}^{n} \vec x_{m}\vec x_{1}&amp;  \sum _{i=1}^{n} \vec x_{m}\vec x_{2}&amp;  \cdots &amp; \sum _{i=1}^{n} \vec x_{m}\vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>再设一个矩阵X：</p>

<p>\[ X =  \left[ \begin{matrix} \vec x_{1}\\  \vec x_{2}\\  \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp; \vec x_{2}&amp; \cdots &amp; \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>于是有：</p>

<p>\[ C = \frac {1}{n}XX^{T} \]</p>

<p><strong>总结下</strong>，对符合均匀分布的、且均值等于0的\(\vec x_{i, 1\leq i \leq m}\)，它的协方差矩阵如下：</p>

<p>\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp; \vec x_{2}&amp; \cdots &amp; \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>\[ C = \frac {1}{n}XX^{T} \]</p>

<p>为了下文能继续推导，需要把C记为\( C_{x} \)。</p>

<h1>PART II</h1>

<p>根据上面得到的协方差矩阵的公式，可以知道：</p>

<ul>
<li><p>\( C_{x} \)是一个对称方阵</p></li>
<li><p>\( C_{x} \)的对角线上的元素分别代表了对某个特征的n次测量的方差</p></li>
<li><p>\( C_{x} \)的非对角线上的元素代表了任意2个特征之间的协方差</p></li>
</ul>

<p>开始进入到PCA的环节。PCA的目标是提炼出\( C_{x} \)的关键信息并剔除冗余信息，这个过程用线性代数表示就是：</p>

<p>\[ Y  = PX  \]</p>

<p>这里面P就是我们需要的目标矩阵。而关于矩阵Y的协方差矩阵\( C_{y} \)的特性是：</p>

<ul>
<li><p>\( C_{y} \)的对角线上的元素(方差)尽可能大（增大信号）</p></li>
<li><p>\( C_{y} \)的非对角线上的元素(协方差)应该等于零，因此\( C_{y} \)还是一个对角线矩阵</p></li>
</ul>

<p>\( C_{x} \)、\( C_{y} \)、P的关系是：</p>

<p>\[  C_{y} =  \frac {1}{n}YY^{T} \]</p>

<p>\[  = \frac {1}{n}(PX)(PX)^{T} \]</p>

<p>\[  = \frac {1}{n}PXX^{T}P^{T} \]</p>

<p>\[  = P(\frac {1}{n}XX^{T})P^{T} \]</p>

<p>\[  = PC_{x}P^{T} \]</p>

<p>即：</p>

<p>\[   C_{y} = PC_{x}P^{T} \]</p>

<p>PCA的求解方法多种多样，下面展示最经典的解法——特征值分解。</p>

<h2>基于特征值分解的PCA</h2>

<p>在<a href="http://127.0.0.1:4000/linear-algebra-6/">矩阵的特征值、特征向量、特征矩阵、迹、特征值分解</a>一文中提到了对称方阵的特征值分解公式：</p>

<p>\[ A =  S\Lambda S^{-1} = S\Lambda S^{T} \]</p>

<p>矩阵\( \Lambda \)是对角矩阵，对角线上的元素为特征值；矩阵S是一个行向量为特征向量的矩阵，\( S^{-1} = S^{T} \)。</p>

<p>有了这个公式后，立即可以知道对称矩阵\( C_{x} \)的特征值分解(对角化)为：</p>

<p>\[ C_{x} =  S\Lambda S^{T} \]</p>

<p>将其代入上面的公式，有:</p>

<p>\[C_{y} = PS\Lambda S^{T}P^{T}   \]</p>

<p>这里可以大胆做个假设：\( P \equiv  S^{T} \)。又因为 \( S^{-1} = S^{T} \)， 则有:</p>

<p>\[ P^{-1} =  (S^{T})^{-1} = (S^{-1})^{-1} = S = P^{T} \]</p>

<p>所以：</p>

<p>\[ C_{y} = PS\Lambda S^{T}P^{T} = PP^{T} \Lambda PP^{T} = PP^{-1} \Lambda PP^{-1} = \Lambda   \]</p>

<p>总结：</p>

<p><strong>当\( P \)(主成分矩阵)是\( C_{x} \)的特征向量矩阵\( S \)时，\( C_{y} \)是特征值矩阵\( \Lambda\)</strong>。</p>

<h2>基于奇异值分解的PCA</h2>

<p>关于SVD的讨论我都会追加<a href="http://127.0.0.1:4000/linear-algebra-9/">线性代数之奇异值(SVD)分解</a>一文中，在这里不展开对SVD的详细讨论。</p>

<p>SVD分解公式：</p>

<p>\[ M = UΣV^{*} \]</p>

<p>（其中各个矩阵的含义就不赘述了）</p>

<p>和PCA最相关的SVD其中一条性质：</p>

<p>\[ M^{*}M = V\Sigma ^{*}U^{*}U\Sigma V^{*} = V\Sigma ^{*}(U^{*}U)\Sigma V^{*} = V\Sigma ^{*}\Sigma V^{*} = V(\Sigma  ^{*}\Sigma )V^{-1}  \]</p>

<p>当元素是实数时，有：</p>

<p>\[ M^{T}M = V(\Sigma  ^{T}\Sigma )V^{-1}  \]</p>

<p>注意，这个公式包含了一个事实：\( M^{T}M \)的特征值分解等于\( V(\Sigma  ^{T}\Sigma )V^{-1}  \)。</p>

<p>现在，假设有一个矩阵Y，Y满足：</p>

<p>\[ Y = \frac { 1 }{ \sqrt {n} }X^{T} \]</p>

<p>那么有：</p>

<p>\[ Y^{T}Y = (\frac { 1 }{ \sqrt {n} }X^{T})^{T}\frac { 1 }{ \sqrt {n} }X^{T} \]</p>

<p>\[ = \frac { 1 }{ \sqrt {n} }\frac { 1 }{ \sqrt {n} } (X^{T})^{T}X^{T} \]</p>

<p>\[ = \frac { 1 }{ n }XX^{T} \]</p>

<p>\[ = C_{x} \]</p>

<p>而Y的SVD分解为：</p>

<p>\[ Y = UΣV^{*} \]</p>

<p>所以有：</p>

<p>\[ Y^{T}Y = V(\Sigma  ^{T}\Sigma )V^{-1} = C_{x} \] </p>

<p>这时候事情又和特征值分解联系上了，在上一小节我们已经知道<strong>当\( P \)是\( C_{x} \)的特征向量矩阵\( S \)时，\( C_{y} \)是特征值矩阵\( \Lambda\)</strong>，而在这个式子中，\(V\)就是\( C_{x} \)的特征向量矩阵！</p>

<p>整理一下这个解法的思路：</p>

<ol>
<li><p>先设\( Y = \frac { 1 }{ \sqrt {n} }X^{T} \)</p></li>
<li><p>对矩阵Y做SVD分解，得到矩阵V，V就是关于X的主成分矩阵</p></li>
</ol>

<h1>参考资料</h1>

<p>A Tutorial on Principal Component Analysis, Jonathon Shlens</p>

    </div>
    <div class="entry">
        (未经授权禁止转载)
    </div>
    <div class="date">
        Written on May 30, 2016
    </div>
    <p>博主将十分感谢对本文章的任意金额的打赏^_^</p>
    <img src="../images/dashang1.jpeg" />
    <img src="../images/dashang2.jpeg" />
    
    
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'qiujiawei';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>



      </section>
    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:voyagingmk@gmail.com"><i class="svg-icon email"></i></a>


<a href="http://github.com/barryclark/jekyll-now"><i class="svg-icon github"></i></a>




<a href="http://twitter.com/voyagingmk"><i class="svg-icon twitter"></i></a>


        </footer>
      </div>
    </div>

  </body>
</html>
